{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21112d57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.20\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a217c43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: datasets in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (2.21.0)\n",
      "Requirement already satisfied: transformers in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (4.45.2)\n",
      "Collecting rouge-score\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e2/c5/9136736c37022a6ad27fea38f3111eb8f02fe75d067f9a985cc358653102/rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jieba in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (0.42.1)\n",
      "Requirement already satisfied: filelock in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: packaging in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: absl-py in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: click in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=e7939d9a4f5bc06a6d0ac6d8573d5790d932288434a79bb3980ded8419b14b53\n",
      "  Stored in directory: /Users/wuyinghui/Library/Caches/pip/wheels/88/8d/5a/cbcddcc5d2fae2c2f8d83a91c3f5205afee148935880a9c29b\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers rouge-score jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce748804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: ipywidgets in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (8.1.5)\n",
      "Requirement already satisfied: tqdm in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (4.66.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipywidgets) (8.12.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: backcall in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pickleshare in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: appnope in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook run server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade ipywidgets tqdm\n",
    "! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee9adfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "import tqdm \n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files='nlpcc_data.json', field='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214e4f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b16fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(example):\n",
    "    return {\n",
    "        \"document\": example[\"content\"],\n",
    "        \"summary\": example[\"title\"],\n",
    "        \"id\":\"0\"\n",
    "    }\n",
    "dataset = dataset[\"train\"].map(flatten, remove_columns=[\"title\", \"content\"]) # , remove_columns=[\"title\", \"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89636f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenModel = \"bert-base-chinese\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TokenModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c468f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/bart-base-cnn\"\n",
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\" # BART-12-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a6a9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024 # input, source text\n",
    "max_target_length = 256 # summary, target text\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43405ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'summary', 'id'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f2b6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = dataset\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ed62e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb7862466904a4d99f6cb30c0eb6c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3f37d9162242aea5fb5bfa6a9fa9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fefd80c988a4a6aa23fc0a4d43a2b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import dataset_dict\n",
    "import datasets\n",
    "\n",
    "train_data_txt, validation_data_txt = dataset.train_test_split(test_size=0.1).values()\n",
    "train_data_txt, test_data_tex = train_data_txt.train_test_split(test_size=0.1).values()\n",
    "# 装载数据\n",
    "dd = datasets.DatasetDict({\"train\":train_data_txt,\"validation\": validation_data_txt,\"test\":test_data_tex }) \n",
    "\n",
    "raw_datasets = dd\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0870df27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 40500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 4500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa338e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f8746a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>菲律宾总统贝尼尼奥·阿基诺三世访问日本期间就南海问题大放厥词,言辞充满挑衅,引发中方强烈不满。这一次,连菲律宾本国媒体都看不下去了,厉声告诫阿基诺找回理智,不要玩火自焚。【莫要玩火,小心自焚】新华国际客户端注意到,菲律宾《马尼拉时报》4日发表社论,题为《总统对中国发起不必要的挑衅》。文章说,阿基诺访日期间发表演讲,提及争议领土问题(指南海问题),再次搅动时局。文章说:“这种对中国的公然挑衅看起来毫无必要,也不清楚他到底希望借此维护什么国家利益。”日本媒体报道,阿基诺把中国在南海的行为与二战前的纳粹德国相提并论,同时呼吁美国作为超级大国在南海问题上发挥作用。社论说,菲律宾政府的行为实际上令南海局势趋于恶化。美国增加了在这一地区的海军存在,对中国的言辞也更加激烈。“我们只盼望阿基诺总统心里有谱,能理智地终结这个局面。难道按照他的期望,美国海军在争议领土附近的存在能吓走中国?”社论问道。文章说,中国反而有可能强化立场,“我们怀疑菲律宾是否已经准备好应对那样的局面”。社论告诫阿基诺政府不要玩火自焚。“难道菲律宾期望中国和美国围绕争议领土爆发一场热战?我们希望不会,因为当两个巨人碰撞时,小个子如菲律宾可能会被压扁。”更重要的是,亚太地区的不稳定将威胁到整个地区的经济发展,继而殃及菲律宾。社论说,菲律宾与中国的关系不应由领土争端“界定”。相比对中国采取不必要的对抗、致使中美关系紧张,与中国保持友好关系,菲律宾反而能够获得更多。“我们相信能够经由外交手段和平化解冲突。为了与中国达成和平互利的解决方案,阿基诺总统应该记住一句老话,‘用蜂蜜捉苍蝇,总比用醋强’。”【丢掉幻想,回头是岸】针对阿基诺关于南海问题的言论,外交部发言人华春莹3日在例行记者会上表示,中方对菲方领导人的“荒谬无理言论”深感震惊并强烈不满。“我再次严肃正告菲方某些人丢掉幻想,回头是岸,停止挑拨挑衅,回到通过双边渠道谈判协商解决争议的正确轨道上来。”华春莹说。华春莹说,回顾一下南海争议的事实经纬不难发现,上世纪70年代以来武力非法侵占中国南沙群岛部分岛礁的是菲律宾;1999年以来企图以“坐滩”形式窃占中国仁爱礁的是菲律宾;2012年派遣军舰武力袭扰在中国黄岩岛海域正常作业的中国渔船、渔民的是菲律宾;2013年无视中方作为《联合国海洋法公约》缔约国应有的权利、违背《南海各方行为宣言》和两国间一系列共识,单方面将有关争议提交所谓国际仲裁的是菲律宾;近年来出于一己私利不断勾结域外国家搅浑水、抹黑攻击中国的还是菲律宾。她表示,中国是重信守诺的负责任国家,始终致力于通过同直接当事国之间的谈判协商解决有关争议,同时,中国政府维护领土主权和海洋权益的意志“坚定不移”。(记者杨天沐,编辑韩梁、胡若愚,新华国际客户端独家报道)</td>\n",
       "      <td>阿基诺访日时演讲提及南海问题,菲媒称其对中国公然挑衅毫无必要,告诫其找回理智,不要玩火自焚</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>本报讯(记者党文民)7月26日14时,省气象台发布干旱橙色预警,预计未来7天我省无大范围降水天气过程,干旱将进一步发展,请干旱地区做好防范。根据综合气象干旱指数监测,截至7月25日,全省有47%的县市气象干旱达到重旱等级,有17%的县市达到特旱等级。重旱以上的区域包括郑州、开封、漯河、平顶山、周口、焦作、商丘、许昌8个省辖市和三门峡、洛阳、南阳、驻马店、新乡5个省辖市的部分区域,其中特旱区分布在开封、许昌、平顶山、周口、焦作。今年6月份以来,我省平均降水量比往年同期大幅减少。省气象局气候中心高级工程师张善强说,这是由于今年的南海季风爆发时间偏晚,携带水汽偏少,而控制我省的副热带高压今年的位置偏南偏西,也阻断了水汽向北输送,加上冷空气势力较弱,因此在我省形成有效降水的几率大大降低。据了解,干旱预警信号分为橙色、红色两个级别。橙色预警的标准为:预计未来一周综合气象干旱指数达到重旱(气象干旱为25~50年一遇),或者某一县(区)有40%以上的农作物受旱。此时,有关部门应及时采取启用应急备用水源,压减城镇供水指标,限制非生产性高耗水及服务业用水等防御措施。</td>\n",
       "      <td>河南发布干旱橙色预警,未来7天无大范围降水,全省47%县市达到重旱等级。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>昨天,在浙江宁波一家超市,因其没有进问题批次的货,所以还在正常销售雅培幼儿喜康力(3段)900克/罐装奶粉。图/CFP昨天,国家质检总局表示,我国已对恒天然集团浓缩乳清蛋白粉和奶粉基粉两种原料无限期停止进口,直到污染事件完全解决。恒天然方面则确认雅培为事先要求不公布的企业。另外,新西兰总理表示,或将考虑为“毒奶粉”事件赴华。□说法■恒天然雅培为最后一家牵连企业据恒天然的公关公司提供的5日中午发布会的现场速记称,此次涉事的38吨浓缩乳清蛋白粉中有18吨为恒天然自己分别在澳大利亚和新西兰的工厂使用,最主要用来生产基粉,用于生产婴幼儿营养品。其中,澳大利亚工厂生产的产品涉及到两家生产婴儿营养制品的公司,除了已公开的拥有多美滋、可瑞康等品牌的达能外,另外一家公司则暂时不让恒天然提及它们的名字。那么这家不愿意提及名称的企业是否就是雅培?对此,雅培方面昨天凌晨接受本报采访时给予了否认。其表示,首先雅培并未使用可能受到污染的原料,另外,雅培产品是在新西兰实施包装的,而不是在澳大利亚包装的。不过,昨天晚间,恒天然有关负责人郝晓红则向记者表示,这家不愿意提及名称的企业就是雅培,雅培的产品原料确实未涉及到这38吨问题浓缩乳清蛋白粉,只是受到了生产线的影响。澄清两天内是“启动召回”8月5日中午,恒天然首席执行官西奥·史毕根斯在北京的媒体发布会上表示:恒天然未来48小时内相关产品将会得到召回。对此说法,有消息引用涉事企业的表态称“这是不可能完成的任务”。而按照“48小时内相关产品得到召回”的说法,就意味着今天中午将是召回时间的最后时限。不过对于这种说法,恒天然方面有关负责人郝晓红昨天接受记者采访时则澄清表示:发布会现场可能存在翻译和速记的误解,48小时内指的是“启动召回和进行召回措施”,而非“完成召回”。至于何时完成召回,则需要一个过程和时间。郝晓红还表示,到8月6日凌晨雅培公布防御性召回措施时,所有涉事的企业名单得到了公布,其实在24小时内就全面启动了对相关产品的召回。■雅培涉事产品北京极少量有售在恒天然发布会结束后,昨天凌晨,此前一直表示自己的原料并未涉及到恒天然这批问题浓缩乳清蛋白粉的雅培也宣布,对两批次幼儿喜康力(3段)奶粉进行“预防性回收”并销毁,召回产品共7181箱,其中约112箱已售出。召回范围内的产品批号分别为287834K402和287844K402,生产日期均为2013年5月2日,失效日期为2015年10月31日。雅培特别强调,雅培供应中国市场的所有产品均未使用恒天然受污染的乳清蛋白粉,但经恒天然方面8月4日晚间确认,上述2个批次供应中国市场的雅培金装幼儿喜康力听装奶粉在恒天然公司包装线上实施包装,但该包装生产线在使用有问题原料后未经彻底清洗即开始包装雅培产品。雅培方面昨天表示,目前无法提供涉事产品完全准确的销售地点,但确实有进入北京市场,不过数量很少,大约为5箱以内。■多美滋新增加两个召回奶粉批次卷入恒天然涉毒浓缩乳清蛋白粉的多美滋昨天也再次发声明称,召回的产品由此前的12个批次增加到14个批次。多美滋声明显示,新增召回的2个奶粉批次为多领加延续较大婴幼儿配方奶粉(2阶段)(2013年5月23日之后生产)1200g,批号分别为1F3199、1F3208。与此同时,多美滋还更改了一个批次产品的相关信息,此前公布召回的优阶贝护延续较大婴儿配方2阶段奶粉850g+50g的1H3172,改成900g产品的1H3172。多美滋方面表示,新增的两个批次是根据恒天然8月5日的新增信息,这2个批次的多领加2阶段产品有可能受到影响。多美滋启动预防性召回。对此,多美滋方面称,因为此前1H3172批次的净重被混淆,调整是为了纠正此前的工作失误。此前的8月4日,多美滋表示,已查明部分优阶贝护和多领加2阶段产品有可能受到影响,共涉及12个批次。其中流入市场的问题多美滋奶粉有420.188吨。如今新增两批次产品,多美滋方面昨天晚间接受记者采访时确认,相关产品有流入市场,但具体的量暂时还没有统计数据,届时以相关部门公布数据为准。不过公司已正采取措施迅速下架这两个批次的产品。□发布■国家质检总局无限期停进口涉事两原料昨天上午,国家质检总局发出通告,要求雅培公司召回两个可能被污染的批次奶粉。另外,目前我国已对恒天然集团浓缩乳清蛋白粉和奶粉基粉两种原料无限期停止进口,直到污染事件完全解决。目前,要根据受污染的乳清蛋白粉的范围和市场流向的掌握情况以及恒天然集团的报告来决定何时恢复进口。■国家食品药品监督管理总局生产婴幼儿配方乳粉企业生产条件审查细则征意见为方便广大消费者、食品监管部门和社会各界查询信息,鼓励广大消费者和社会各界参与婴幼儿配方乳粉质量安全监督,国家食品药品监督管理总局6日对外公布各省级食品监管部门批准取得食品生产许可证的128家婴幼儿配方乳粉生产企业的相关信息。这128家企业包括美赞臣营养品(中国)有限公司、多美滋婴幼儿食品有限公司、雅培(广州)营养品有限公司、内蒙古伊利实业集团股份有限公司等,具体名单可在国家食品药品监督管理总局网站查询。此外,由国家食品药品监管总局组织专家、食品安全监督部门人员研究、修订的《企业生产婴幼儿配方乳粉许可条件审查细则(2013版)(征求意见稿)》,现面向全社会公开征求意见。据了解,新的细则征求意见稿参照药品管理全面提高了婴幼儿配方乳粉生产企业生产、管理要求。□表态新西兰总理考虑赴华为应对乳制品巨头恒天然集团产品受肉毒杆菌污染事件所引发的“信任危机”,新西兰政府出面公关。总理约翰·基6日说,如果有必要,他将为此亲赴中国。约翰·基接受《新西兰先驱报》采访时说,新西兰外交部长默里·麦卡利将在数周内前往中国,贸易部长蒂姆·格罗泽也可能访华,以协助解决“毒奶粉”危机。他说:“如有必要,我也会去(中国)。”约翰·基说,他打算与来访的中国食品监管部门官员会面,双方将保持对话。综合京华时报记者胡笑红王晟实习记者颜榛新华社</td>\n",
       "      <td>恒天然确认雅培为事先要求不公布的企业,称其为最后一家牵连企业</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>边飞等了一年半,河北省大名县原县委书记边飞终于等来了他的判决:死刑,缓期二年执行,剥夺政治权利终身,并处没收个人全部财产。1.01亿元,是县委书记边飞的涉案数额,这比原铁道部长刘志军的涉案的6460万还要多出3000余万元。自从2013年12月被纪检监察机关通报涉嫌严重违纪违法,边飞就已消失在公众视野中多日,这次以“亿元巨贪”的名头重新“归来”,不禁勾起政知圈(微信ID:wepolitics)小编的好奇心,这是一个什么样的县委书记?8年赚了上亿元一位曾在邯郸工作过的官场人士对政知圈小编表示,边飞涉案金额巨大,在他们公务员系统内部确实让人震惊。他还透露,像边飞任职大名县委书记时,工资也就5000元左右。然而看看石家庄市中级人民法院出具的一审判决,边飞的敛财非同一般。判决书显示,2005年3月至2013年10月,边飞在河北省邯郸市魏县、永年县、大名县担任县委书记期间,利用其职务上的便利,为他人在职务晋升调整、项目协调审批、工程承揽建设等方面谋取利益,先后多次非法收受、索取他人贿赂,共计折合人民币5920余万元。另有价值人民币4190余万元巨额财产不能说明来源。此外,边飞滥用职权给国家造成财产流失776万余元。从一审判决来看,边飞涉及三项罪名,分别是受贿罪、巨额财产来源不明罪和滥用职权罪。值得一提的是,法院出具的一审判决书还显示,边飞有索贿情节。贪腐逾亿元,为何能获死缓?审理此案的石家庄中级人民法院给出了判决依据:由于其在侦查期间能够主动交代犯罪事实,所得赃款赃物已基本追缴或抵缴,具有法定、酌定从轻处罚情节。根据边飞犯罪的事实、性质、情节和对社会的危害程度,法院依法作出上述判决。18年“县官”从被调查到一审宣判,边飞案已经历时一年半时间。如今他的名字在其主政过的政府网站已经难觅踪迹,但在河北新闻网上,政知圈小编还是找到了一份关于他较为完整的简历。这份简历显示,今年已经54岁的边飞是土生土长的河北人,出生于保定市所辖的顺平县,17岁参加工作。他主要从政经历都是在邯郸境内,1995年到2003年八年时间里,先后在三个县担任过副县长、县委副书记和县长;2003年到2013年十年间,历任邯郸所辖的魏县、永年县、大名县三个县县委书记。2009年在永年县任县委书记期间,他的级别又之前的正处级擢升为副厅级。最后,他在“副厅级”级别职务的任上落马。一位接近当地官场的知情人士向政知圈(微信ID:wepolitics)小编透露,边飞也是“官二代”,父亲曾是河北当地一名副专员级官员。可惜儿子不能像老子那样,在官场上体面地退休。主政国家级贫困县从判决书来看,边飞的问题出在其历任三个县县委书记之时。而这三个县里,就有大名县和魏县两个县分别位列国家扶贫办2001年和2012年发布的两份“国家级贫困县”名录。一位邯郸当地官员告诉政知圈小编,这三个县中,永年是经济实力最强的,前些年以“标准件”产业为支柱,京广线上都可以看到永年标准件集散地的标志;而身为国家级贫困县的大名和魏县基本上没有什么工业,是以农业为主。既然边飞主政的两个地方都是国家级贫困县,他又是靠什么敛财的呢?边家“夫妻店”根据邯郸当地媒体报道,2009年以来,永年县经历了一场当时“风靡”河北的“三年大变样”战略规划。而其间,边飞正担任永年县委书记。报道显示,2010年被称为河北城镇面貌三年大变样的攻坚之年、决战之年。当年,永年县已累计完成拆迁8.44万平方米。而这里日后建起来的正是2013年永年县委九届五次全会上的工作报告中提到的“洺州新城、中央商务区、滨河新区”。一位接近永年官场的知情人士透露,边飞的落马跟永年这场旧城改造建设有关。边飞被调查之后,永年县数个局长也被“调查”。这位知情人士还讲述了一个细节,当年,边飞的妻子垄断了永年的建筑土石方开挖,基建项目如果不让边飞妻子参与,项目就很难做下去。石家庄检察院曾通报,2008年至2011年,边飞在担任永年县委书记期间,滥用职权,非法返还某公司776.1万元土地出让金,给国家财产造成重大经济损失。边飞当官,身边人也捞到了好处。有媒体曾从河北省检察院获悉,王某自1998年初至2013年10月一直任边飞专职司机,其间被转干。自2012年5月起,王某利用影响力受贿50万元,并在得知边飞被组织调查后,于2013年11月按照边飞事前吩咐,将其藏匿的现金、金条、玉器等价值数千万元的财物转移藏匿。2014年8月,王某因涉嫌构成利用影响力受贿罪和掩饰、隐瞒犯罪所得罪,被石家庄市井陉县检察院向井陉县人民法院提起公诉。如今,“夫妻店”已成往事。身陷囹圄的边飞,只能遥想当年的名利了。</td>\n",
       "      <td>大名原县委书记边飞8年贪腐上亿,工资约5千元;所主政两地都系国家级贫困县,其妻垄断建筑土石方开挖。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“他吞牙刷了!”前天晚上6点多,一位患者被紧急送往大连大学附属中山医院急诊室。据了解,患者于当天下午3点左右吞了一支牙刷。让医生们惊讶的是,这已经不是他第一次做这种事了。据了解,患者今年47岁,在我市一家疗养院里住。前天下午三点,他吞下了牙刷,第一次吞失败了,牙刷柄朝下,牙刷头在上面,怎么也没吞下去。之后他拽出牙刷,将牙刷头朝下,牙刷柄朝上,再次吞牙刷,使劲一吞,牙刷断裂开来。当疗养院的人发现后立刻将他送到了医院。急诊科医生庞洪刚马上给患者拍片子,发现牙刷头已经掉进了结肠里,而牙刷柄还卡在患者的食道中。之后医生又给患者做CT,确定了牙刷头和牙刷柄的位置。紧接着胃镜科的医生手术将卡在食道中的牙刷柄取了出来。而留在结肠里的牙刷头医生并没有着急取出。庞医生说,最好牙刷头能通过排便排出体外,这样患者能少遭点罪。实在不行也只能进行手术取出了,现在患者仍在进一步观察中。令医生吃惊的是,这已经不是他第一次吞牙刷了。之前他还吞过两次牙刷,好在都被及时发现。每次吞牙刷进医院时,他的家属都会来看望他。在医院里,医生问他为什么要吞牙刷,他回答:“每次吞,家人都能来看我。”患者的回答让医生一阵唏嘘。(大连晚报)</td>\n",
       "      <td>大连47岁男子住疗养院三次吞牙刷 称这样家人能来看望</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf4eb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: transformers in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772afe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f772bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Collecting fire\n",
      "  Using cached fire-0.7.0-py3-none-any.whl\n",
      "Requirement already satisfied: termcolor in /Users/wuyinghui/opt/anaconda3/envs/llm/lib/python3.8/site-packages (from fire) (2.4.0)\n",
      "Installing collected packages: fire\n",
      "Successfully installed fire-0.7.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c3849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import fire\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, PreTrainedModel\n",
    "from transformers.utils import logging\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c74bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_layers(src_layers: nn.ModuleList, dest_layers: nn.ModuleList, layers_to_copy: List[int]) -> None:\n",
    "    layers_to_copy = nn.ModuleList([src_layers[i] for i in layers_to_copy])\n",
    "    assert len(dest_layers) == len(layers_to_copy), f\"{len(dest_layers)} != {len(layers_to_copy)}\"\n",
    "    dest_layers.load_state_dict(layers_to_copy.state_dict())\n",
    "\n",
    "\n",
    "LAYERS_TO_COPY = {\n",
    "    # maps  num layers in teacher -> num_layers in student -> which teacher layers to copy.\n",
    "    # 12: bart, 16: pegasus, 6: marian/Helsinki-NLP\n",
    "    12: {\n",
    "        1: [0],  # This says that if the teacher has 12 layers and the student has 1, copy layer 0 of the teacher\n",
    "        2: [0, 6],\n",
    "        3: [0, 6, 11],      # the first, 7th and 12th decode layers\n",
    "        4: [0, 4, 8, 11],\n",
    "        6: [0, 2, 4, 7, 9, 11],\n",
    "        9: [0, 1, 2, 4, 5, 7, 9, 10, 11],\n",
    "        12: list(range(12)),\n",
    "    },\n",
    "    16: {  # maps  num layers in student -> which teacher layers to copy\n",
    "        1: [0],\n",
    "        2: [0, 15],\n",
    "        3: [0, 8, 15], \n",
    "        4: [0, 5, 10, 15],\n",
    "        6: [0, 3, 6, 9, 12, 15],\n",
    "        8: [0, 2, 4, 6, 8, 10, 12, 15],\n",
    "        9: [0, 1, 3, 5, 7, 9, 11, 13, 15],\n",
    "        12: [0, 1, 2, 3, 4, 5, 6, 7, 9, 11, 13, 15],\n",
    "        16: list(range(16)),\n",
    "    },\n",
    "    6: {1: [0], 2: [0, 5], 3: [0, 2, 5], 4: [0, 1, 3, 5], 6: list(range(6))},\n",
    "}\n",
    "LAYERS_TO_SUPERVISE = {\n",
    "    # maps  num layers in student -> which teacher layers to copy.\n",
    "    6: {1: [5], 2: [3, 5], 3: [1, 4, 5], 4: [1, 2, 4, 5]},\n",
    "    12: {1: [11], 2: [5, 11], 3: [3, 7, 11], 6: [1, 3, 5, 8, 10, 11]},\n",
    "    16: {1: [15], 4: [4, 9, 12, 15], 8: [1, 3, 5, 7, 9, 11, 13, 15]},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ac00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_student_by_copying_alternating_layers(\n",
    "    teacher: Union[str, PreTrainedModel],\n",
    "    save_path: Union[str, Path] = \"student\",\n",
    "    e: Union[int, None] = None,\n",
    "    d: Union[int, None] = None,\n",
    "    copy_first_teacher_layers=False,\n",
    "    e_layers_to_copy=None,\n",
    "    d_layers_to_copy=None,\n",
    "    **extra_config_kwargs\n",
    ") -> Tuple[PreTrainedModel, List[int], List[int]]:\n",
    "    \n",
    "    _msg = \"encoder_layers and decoder_layers cannot be both None-- you would just have an identical teacher.\"\n",
    "    assert (e is not None) or (d is not None), _msg\n",
    "    if isinstance(teacher, str):\n",
    "        AutoTokenizer.from_pretrained(teacher).save_pretrained(save_path)  # purely for convenience\n",
    "        teacher = AutoModelForSeq2SeqLM.from_pretrained(teacher).eval()\n",
    "    else:\n",
    "\n",
    "        assert isinstance(teacher, PreTrainedModel), f\"teacher must be a model or string got type {type(teacher)}\"\n",
    "    init_kwargs = teacher.config.to_diff_dict()\n",
    "\n",
    "    try:\n",
    "        teacher_e, teacher_d = teacher.config.encoder_layers, teacher.config.decoder_layers\n",
    "        if e is None:\n",
    "            e = teacher_e\n",
    "        if d is None:\n",
    "            d = teacher_d\n",
    "        init_kwargs.update({\"encoder_layers\": e, \"decoder_layers\": d})\n",
    "    except AttributeError:  # T5\n",
    "        teacher_e, teacher_d = teacher.config.num_layers, teacher.config.num_decoder_layers\n",
    "        if e is None:\n",
    "            e = teacher_e\n",
    "        if d is None:\n",
    "            d = teacher_d\n",
    "        init_kwargs.update({\"num_layers\": e, \"num_decoder_layers\": d})\n",
    "\n",
    "    # Kwargs to instantiate student: teacher kwargs with updated layer numbers + **extra_config_kwargs\n",
    "    init_kwargs.update(extra_config_kwargs)\n",
    "\n",
    "    # Copy weights\n",
    "    student_cfg = teacher.config_class(**init_kwargs)\n",
    "    student = AutoModelForSeq2SeqLM.from_config(student_cfg)\n",
    "    # Start by copying the full teacher state dict this will copy the first N teacher layers to the student.\n",
    "    info = student.load_state_dict(teacher.state_dict(), strict=False)\n",
    "    assert info.missing_keys == [], info.missing_keys  # every student key should have a teacher keys.\n",
    "\n",
    "    if copy_first_teacher_layers:  # Our copying is done. We just log and save\n",
    "        e_layers_to_copy, d_layers_to_copy = list(range(e)), list(range(d))\n",
    "        logger.info(\n",
    "            f\"Copied encoder layers {e_layers_to_copy} and decoder layers {d_layers_to_copy}. Saving them to {save_path}\"\n",
    "        )\n",
    "        student.save_pretrained(save_path)\n",
    "        return student, e_layers_to_copy, d_layers_to_copy\n",
    "\n",
    "    # Decide which layers of the teacher to copy. Not exactly alternating -- we try to keep first and last layer.\n",
    "    if e_layers_to_copy is None:\n",
    "        e_layers_to_copy: List[int] = pick_layers_to_copy(e, teacher_e)\n",
    "    if d_layers_to_copy is None:\n",
    "        d_layers_to_copy: List[int] = pick_layers_to_copy(d, teacher_d)\n",
    "\n",
    "    try:\n",
    "        copy_layers(teacher.model.encoder.layers, student.model.encoder.layers, e_layers_to_copy)\n",
    "        copy_layers(teacher.model.decoder.layers, student.model.decoder.layers, d_layers_to_copy)\n",
    "    except AttributeError:  # For t5, student.model.encoder.layers is called student.encoder.block\n",
    "        copy_layers(teacher.encoder.block, student.encoder.block, e_layers_to_copy)\n",
    "        copy_layers(teacher.decoder.block, student.decoder.block, d_layers_to_copy)\n",
    "    logger.info(\n",
    "        f\"Copied encoder layers {e_layers_to_copy} and decoder layers {d_layers_to_copy}. Saving them to {save_path}\"\n",
    "    )\n",
    "    student.config.init_metadata = dict(\n",
    "        teacher_type=teacher.config.model_type,\n",
    "        copied_encoder_layers=e_layers_to_copy,\n",
    "        copied_decoder_layers=d_layers_to_copy,\n",
    "    )\n",
    "    student.save_pretrained(save_path)\n",
    "    # Save information about copying for easier reproducibility\n",
    "\n",
    "    return student, e_layers_to_copy, d_layers_to_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3add8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_layers_to_copy(n_student, n_teacher):\n",
    "    try:\n",
    "        val = LAYERS_TO_COPY[n_teacher][n_student]\n",
    "        return val\n",
    "    except KeyError:\n",
    "        if n_student != n_teacher:\n",
    "            warnings.warn(\n",
    "                f\"no hardcoded layers to copy for teacher {n_teacher} -> student {n_student}, defaulting to first {n_student}\"\n",
    "            )\n",
    "        return list(range(n_student))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb003d76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, list_en, list_de \u001b[38;5;241m=\u001b[39m create_student_by_copying_alternating_layers(\u001b[43mmodel\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrian.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model, list_en, list_de = create_student_by_copying_alternating_layers(model, 'trian.pth', 12, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9cf07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=1,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=batch_size,  # demo\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    # learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b58a54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa3a3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(jieba.cut(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(jieba.cut(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25d5b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b93fe4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting torch==1.5.0\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/76/58/668ffb25215b3f8231a550a227be7f905f514859c70a65ca59d28f9b7f60/torch-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (752.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 752.0 MB 14.5 MB/s eta 0:00:01MB 68.6 MB/s eta 0:00:11 MB 68.6 MB/s eta 0:00:11\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from torch==1.5.0) (1.20.2)\n",
      "Requirement already satisfied: future in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from torch==1.5.0) (0.18.2)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.5.0 requires torch==1.4.0, but you have torch 1.5.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.5.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-1.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd0e92de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, token_type_ids, document, summary.\n",
      "***** Running training *****\n",
      "  Num examples = 40500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20250' max='20250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20250/20250 1:33:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>12.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>6.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>6.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.921700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>5.257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.773800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.916500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.795400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.688500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.658300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.586300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.631400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.353300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.243100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>4.726900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>4.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>4.306300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>4.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>4.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>4.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>4.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>4.135400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>4.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>4.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>3.998700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>4.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>4.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>4.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>4.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>4.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>4.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>4.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>4.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>4.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>4.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>4.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>4.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>4.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>3.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>4.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>3.909800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>3.926300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>4.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>3.979200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>4.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>3.789500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>3.852700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>3.939300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>3.859500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>3.788400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>3.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>3.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>4.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>3.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>3.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>4.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.968100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>3.951500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>3.989200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>4.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>3.972900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>3.897500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>3.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.783800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>3.964500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>3.985100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>3.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.897200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>3.816700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>4.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>3.855600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>3.827900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>3.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>4.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>3.805700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>3.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>3.956800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.990100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>3.711200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>3.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>3.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>3.829800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>3.838500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>3.737900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>3.842400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>3.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>3.783300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.857600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>3.738200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>3.886100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>3.871300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>3.669100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>3.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>3.821200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>3.816800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>3.809500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>3.736600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.755500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>3.722500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>3.544100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>3.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>3.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>3.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>3.809300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>3.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>3.592600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>3.919400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>3.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>3.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.703400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>4.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>3.708300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>3.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>3.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>3.849100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.591500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>3.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>3.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>3.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>3.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>3.818800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>4.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>3.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>3.793300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>3.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.986700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>3.767900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>3.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>3.829200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>3.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>3.826500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>3.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>3.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>3.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>3.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.894300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>3.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>3.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>3.532500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>3.734600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>3.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>3.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>3.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>3.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>3.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.715200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>3.711400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>3.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>3.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>3.825400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>3.776900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>3.783100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>3.705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>3.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>3.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>3.496500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>3.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>3.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>3.536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>3.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>3.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>3.889500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>3.593800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>3.658900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.574400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>3.661400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>3.748400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>3.596400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>3.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>3.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>3.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>3.541700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>3.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>3.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.488400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>3.702600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>3.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>3.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>3.783900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>3.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>3.655500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>3.638800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>3.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>3.621200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.477600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>3.567100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>3.625500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>3.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>3.554400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>3.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>3.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>3.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>3.522900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>3.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>3.581300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>3.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>3.448700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>3.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>3.658400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>3.651800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>3.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>3.716200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>3.640400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>3.566400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>3.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>3.550100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>3.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>3.580100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>3.632200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>3.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>3.468400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.592900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>3.679500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>3.705500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12650</td>\n",
       "      <td>3.608600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>3.394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>3.553700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>3.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12850</td>\n",
       "      <td>3.423800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>3.536100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12950</td>\n",
       "      <td>3.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13050</td>\n",
       "      <td>3.460500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>3.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13150</td>\n",
       "      <td>3.501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>3.377200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>3.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>3.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13350</td>\n",
       "      <td>3.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>3.638700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13450</td>\n",
       "      <td>3.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13550</td>\n",
       "      <td>3.428200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>3.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13650</td>\n",
       "      <td>3.411300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>3.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>3.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>3.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13850</td>\n",
       "      <td>3.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>3.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13950</td>\n",
       "      <td>3.682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.452400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14050</td>\n",
       "      <td>3.441700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>3.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14150</td>\n",
       "      <td>3.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>3.536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>3.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>3.571700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14350</td>\n",
       "      <td>3.726900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>3.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14450</td>\n",
       "      <td>3.487900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.659400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14550</td>\n",
       "      <td>3.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>3.340400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14650</td>\n",
       "      <td>3.475800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>3.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>3.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>3.615700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14850</td>\n",
       "      <td>3.543400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>3.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14950</td>\n",
       "      <td>3.445400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15050</td>\n",
       "      <td>3.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>3.587000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15150</td>\n",
       "      <td>3.482700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>3.297300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>3.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>3.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15350</td>\n",
       "      <td>3.486000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>3.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15450</td>\n",
       "      <td>3.378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.318400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15550</td>\n",
       "      <td>3.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>3.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15650</td>\n",
       "      <td>3.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>3.378500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15750</td>\n",
       "      <td>3.508800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>3.410600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15850</td>\n",
       "      <td>3.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>3.462800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15950</td>\n",
       "      <td>3.431900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16050</td>\n",
       "      <td>3.457100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>3.583900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16150</td>\n",
       "      <td>3.411400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>3.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16250</td>\n",
       "      <td>3.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>3.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16350</td>\n",
       "      <td>3.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>3.708900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16450</td>\n",
       "      <td>3.599600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16550</td>\n",
       "      <td>3.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>3.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16650</td>\n",
       "      <td>3.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>3.575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16750</td>\n",
       "      <td>3.416900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>3.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16850</td>\n",
       "      <td>3.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>3.299800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16950</td>\n",
       "      <td>3.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.490700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17050</td>\n",
       "      <td>3.326400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>3.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17150</td>\n",
       "      <td>3.556400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>3.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17250</td>\n",
       "      <td>3.556700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>3.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17350</td>\n",
       "      <td>3.446100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>3.331600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17450</td>\n",
       "      <td>3.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17550</td>\n",
       "      <td>3.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>3.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17650</td>\n",
       "      <td>3.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>3.264100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17750</td>\n",
       "      <td>3.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>3.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17850</td>\n",
       "      <td>3.460400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>3.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17950</td>\n",
       "      <td>3.480600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18050</td>\n",
       "      <td>3.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>3.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18150</td>\n",
       "      <td>3.426300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>3.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18250</td>\n",
       "      <td>3.293800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>3.333000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18350</td>\n",
       "      <td>3.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>3.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18450</td>\n",
       "      <td>3.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.435900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18550</td>\n",
       "      <td>3.478400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>3.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18650</td>\n",
       "      <td>3.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>3.540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18750</td>\n",
       "      <td>3.324300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>3.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18850</td>\n",
       "      <td>3.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>3.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18950</td>\n",
       "      <td>3.429100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19050</td>\n",
       "      <td>3.365100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>3.567200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19150</td>\n",
       "      <td>3.411500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>3.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19250</td>\n",
       "      <td>3.480300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>3.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19350</td>\n",
       "      <td>3.562600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>3.339300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19450</td>\n",
       "      <td>3.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19550</td>\n",
       "      <td>3.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19650</td>\n",
       "      <td>3.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>3.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19750</td>\n",
       "      <td>3.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>3.397800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19850</td>\n",
       "      <td>3.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>3.270100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19950</td>\n",
       "      <td>3.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.513500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20050</td>\n",
       "      <td>3.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>3.387700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20150</td>\n",
       "      <td>3.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>3.401800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20250</td>\n",
       "      <td>3.291400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-500\n",
      "Configuration saved in results/checkpoint-500/config.json\n",
      "Model weights saved in results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-1000\n",
      "Configuration saved in results/checkpoint-1000/config.json\n",
      "Model weights saved in results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to results/checkpoint-1500\n",
      "Configuration saved in results/checkpoint-1500/config.json\n",
      "Model weights saved in results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to results/checkpoint-2000\n",
      "Configuration saved in results/checkpoint-2000/config.json\n",
      "Model weights saved in results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-2500\n",
      "Configuration saved in results/checkpoint-2500/config.json\n",
      "Model weights saved in results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-3000\n",
      "Configuration saved in results/checkpoint-3000/config.json\n",
      "Model weights saved in results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-3500\n",
      "Configuration saved in results/checkpoint-3500/config.json\n",
      "Model weights saved in results/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-4000\n",
      "Configuration saved in results/checkpoint-4000/config.json\n",
      "Model weights saved in results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-4500\n",
      "Configuration saved in results/checkpoint-4500/config.json\n",
      "Model weights saved in results/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-5000\n",
      "Configuration saved in results/checkpoint-5000/config.json\n",
      "Model weights saved in results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-5500\n",
      "Configuration saved in results/checkpoint-5500/config.json\n",
      "Model weights saved in results/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-6000\n",
      "Configuration saved in results/checkpoint-6000/config.json\n",
      "Model weights saved in results/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-6500\n",
      "Configuration saved in results/checkpoint-6500/config.json\n",
      "Model weights saved in results/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-7000\n",
      "Configuration saved in results/checkpoint-7000/config.json\n",
      "Model weights saved in results/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-7500\n",
      "Configuration saved in results/checkpoint-7500/config.json\n",
      "Model weights saved in results/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-8000\n",
      "Configuration saved in results/checkpoint-8000/config.json\n",
      "Model weights saved in results/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-8500\n",
      "Configuration saved in results/checkpoint-8500/config.json\n",
      "Model weights saved in results/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-9000\n",
      "Configuration saved in results/checkpoint-9000/config.json\n",
      "Model weights saved in results/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-9500\n",
      "Configuration saved in results/checkpoint-9500/config.json\n",
      "Model weights saved in results/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-10000\n",
      "Configuration saved in results/checkpoint-10000/config.json\n",
      "Model weights saved in results/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-10500\n",
      "Configuration saved in results/checkpoint-10500/config.json\n",
      "Model weights saved in results/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-11000\n",
      "Configuration saved in results/checkpoint-11000/config.json\n",
      "Model weights saved in results/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-11500\n",
      "Configuration saved in results/checkpoint-11500/config.json\n",
      "Model weights saved in results/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-12000\n",
      "Configuration saved in results/checkpoint-12000/config.json\n",
      "Model weights saved in results/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-12500\n",
      "Configuration saved in results/checkpoint-12500/config.json\n",
      "Model weights saved in results/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-13000\n",
      "Configuration saved in results/checkpoint-13000/config.json\n",
      "Model weights saved in results/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-13500\n",
      "Configuration saved in results/checkpoint-13500/config.json\n",
      "Model weights saved in results/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-14000\n",
      "Configuration saved in results/checkpoint-14000/config.json\n",
      "Model weights saved in results/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-14500\n",
      "Configuration saved in results/checkpoint-14500/config.json\n",
      "Model weights saved in results/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-15000\n",
      "Configuration saved in results/checkpoint-15000/config.json\n",
      "Model weights saved in results/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-15500\n",
      "Configuration saved in results/checkpoint-15500/config.json\n",
      "Model weights saved in results/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-16000\n",
      "Configuration saved in results/checkpoint-16000/config.json\n",
      "Model weights saved in results/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-16500\n",
      "Configuration saved in results/checkpoint-16500/config.json\n",
      "Model weights saved in results/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-17000\n",
      "Configuration saved in results/checkpoint-17000/config.json\n",
      "Model weights saved in results/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-17500\n",
      "Configuration saved in results/checkpoint-17500/config.json\n",
      "Model weights saved in results/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-18000\n",
      "Configuration saved in results/checkpoint-18000/config.json\n",
      "Model weights saved in results/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-18500\n",
      "Configuration saved in results/checkpoint-18500/config.json\n",
      "Model weights saved in results/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-19000\n",
      "Configuration saved in results/checkpoint-19000/config.json\n",
      "Model weights saved in results/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-19500\n",
      "Configuration saved in results/checkpoint-19500/config.json\n",
      "Model weights saved in results/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-20000\n",
      "Configuration saved in results/checkpoint-20000/config.json\n",
      "Model weights saved in results/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-18500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20250, training_loss=3.7772197069709685, metrics={'train_runtime': 5608.721, 'train_samples_per_second': 7.221, 'train_steps_per_second': 3.61, 'total_flos': 5.519068250185728e+16, 'train_loss': 3.7772197069709685, 'epoch': 1.0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a810e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5294a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"BART.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6caab61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model.load_state_dict(torch.load('BART.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f604ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29496448",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = \"2005年6月，习近平同志首次提出“红船精神”，将其概括为“开天辟地、敢为人先的首创精神，坚定理想、百折不挠的奋斗精神，立党为公、忠诚为民的奉献精神”，深刻阐述了“红船精神”的丰富内涵、历史地位、时代价值。2017年10月，党的十九大闭幕仅一周，习近平总书记就带领中共中央政治局常委同志，瞻仰上海中共一大会址和浙江嘉兴南湖红船，回顾建党历史，重温入党誓词。习近平总书记在南湖革命纪念馆参观时指出：“我们要结合时代特点大力弘扬‘红船精神’。”“红船精神”是中国革命精神之源，激励着我们党砥砺前行、发展壮大，是我们党立党兴党、执政兴国的宝贵精神财富。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b83e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = generate_summary(test_samples, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ad6d2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused2] 月 睾 习 近 平 同 志 首 次 提 出 红 船 精 神 睾 将 其 概 括 为 开 天 辟 地 、 敢 为 人 先 的 首 创 精 神 睾 立 党 为 公 、 忠 诚 为 民 的 奉 献 精 神 。... 。 现 场 图 ) 组 图 ( 组 图 : ) 图 图 ).. 6 图 : 习 6. 。 组 组 图 ) 组 图 图 ) 。 图 图 组 原 金 金 重 。 现 现 场 上 海 [unused2]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36aeee13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['重庆:妈妈半斤白酒下肚后喂奶灌醉宝宝,孩子吃完奶皮肤发烫、手脚乱动,吐奶后清醒过来。',\n",
       " '许昌住院女子与男护士发生关系 结婚后不如意状告院方',\n",
       " '资阳原市委书记李佳涉嫌受贿罪被逮捕,案件侦查工作正在进行中。',\n",
       " '遵义气象台20时55分发布雷电黄色预警:目前遵义县、桐梓等地出现雷雨天气,预计未来将影响中东部地区。',\n",
       " '海南藏族自治州发布暴雨橙色预警:预计共和地区3小时内降雨量将达25毫米以上且降雨量可能持续,引发洪涝、泥石流、城...',\n",
       " '昆明一母亲买菜留孩子一人在家,2岁半娃娃4楼坠下气息奄奄已不能哭闹说话;孩子目前已送医仍在抢救。',\n",
       " '厦门23岁女生去福州看同学,搭乘出租车后失踪,昨晚该女生手机始终占线',\n",
       " '长江翻船浙江籍乘客儿女:爸妈,求你们再更新一次朋友圈;11名乘客家属到达监利,盼看老人最后一眼',\n",
       " '酒泉市发布道路结冰黄色预警:预计未来72小时内,我市肃州区仍有道路结冰,请注意防范。...',\n",
       " '国资委:唐复平不再担任鞍钢总经理,杨华不再担任鞍钢党委书记等,姚林兼任鞍钢党委书记和总经理。',\n",
       " '新疆边境一线反恐战斗现场曝光:一伙暴徒准备逃往国外参加“圣战”,武警获取情报主动出击,经激烈枪战歼灭6名暴徒。',\n",
       " '歌手陈红遭前夫起诉,被指以军人身份经商;前夫曾将名下9家公司股权半数转让给陈红,现要求收回。 ',\n",
       " '菲考虑解雇16名中国专家 ,称为确保“国家电网安全”,应改聘菲律宾人;报道称中国专家须本周离开。',\n",
       " '外交部发言人:中国赴菲律宾第一批救援人员将于20日启程',\n",
       " '南京给试点汽车尾气管\"戴口罩\" PM2.5浓度降九成',\n",
       " '西甲:梅西内马尔传射,巴萨3-0完胜埃瓦尔,4分优势领跑,下轮将客场迎战皇马。',\n",
       " '厦门:男子扬言携炸弹上公交车 惊动排爆特警',\n",
       " '覃塘区平天山阴坑一处山体因暴雨发生坍塌,致山脚下一工棚被冲垮4人被埋,其中2人获救,另2人失踪。',\n",
       " '招远血案被害人家属至今未获赔偿,生活陷困顿;多次咨询政府未获实质性说法,称觉得自己已成“乞丐”,整天去政府要钱',\n",
       " '延安:“被逼卖处”嫌犯家属否认央求官员帮忙调解,称“封口费与我无关”;当地政府承认未向当事人了解情况',\n",
       " '侯树森不再担任解放军副总参谋长职务,空军将领乙晓光接替其工作',\n",
       " '<教养>《魔法亲亲》、《一口袋的吻》……这些绘本光看名字心里就暖,帮娃克服入园恐惧。',\n",
       " '英国情侣借助在世界各地帮人看管房屋和照顾宠物,4年游历8国;曾照看羊驼,免费入住海岛别墅',\n",
       " '宋承宪晒和两合作女演员亲密照,遭批利用刘亦菲增加知名度;网友痛骂“宋渣”“花心”',\n",
       " '中甲综述:延边大胜继续领跑,青岛德比中能双杀海牛,毅腾逆转升班马,卓尔告负掉出冲超集团',\n",
       " '深圳居住证申领门槛将提高,符合条件持证人可申请公租房或租房补贴。',\n",
       " '清华大学与美国华盛顿大学、微软合作创建全球创新学院,系中国高校首次在美国办学;将在全球招生,联合授予两校学位。',\n",
       " '组图:西安西七路一餐厅今晨发生火灾,造成3人死亡,目前现场已被封闭,原因正在调查中。',\n",
       " '宁波一男子因好奇台风“长”啥样,不顾风雨深入山中围观“灿鸿”,被困水流多时险些丧命,幸被一驴友发现并发出求救信号救下。',\n",
       " '甘孜一女科员享副县级待遇被疑官二代 官方:已49岁',\n",
       " '日首相安倍晋三确定8天访美行程,计划在美国国会发表演说,并前往4个城市;日媒称此次访美“时间之长实属罕见”。 ',\n",
       " '石家庄一化工厂两声巨响后腾起浓烟,现场存有特殊化工原料。',\n",
       " '桂林:男子超市内劫持女顾客,与警察对峙2小时后被擒,该男子今年5月刑满释放',\n",
       " '媒体称美团正与投行接触,计划新一轮10亿美元的融资,此轮融资对美团估值将超150亿美元。',\n",
       " '香港:凌晨3时,大屿山欣澳倒扣湾一摄制组疑遇风浪船只翻侧,1人死亡,其余7人上岸后不知所踪。',\n",
       " '东营率先启动城镇医保普通门诊统筹,参保人员可从72家普通门诊统筹签约医疗机构中选一家签约,每年最高报销1000元。',\n",
       " '重庆一劳务公司以高薪海外务工为诱惑,收取近千打工者中介服务费用后人间蒸发,涉及金额上亿元。',\n",
       " '淄博:为会网友入室盗窃 一个烟头揪出嫌疑人',\n",
       " '外媒称中国或填海造岛引印度不安;此前马尔代夫修宪,允许投资10亿美元以上的外国人拥有该国土地',\n",
       " '保定市民谈承接首都职能:得看人家愿不愿意来',\n",
       " '江西布暴雨黄色预警信号:未来6小时内,萍乡、新余、鹰潭、南昌、宜春、上饶等地将出现短时强降水、雷雨大风等强对流天气。',\n",
       " 'NBA快讯:哈登51分火箭力克国王 考神超级3双难阻3连败',\n",
       " '土耳其一对新婚夫妇在婚宴上款待4000名叙利亚难民,所有资金来自宾客随礼;叙利亚内战以来已有400万国民出国避难',\n",
       " '中国将建不动产统一登记制 专家称有助征税和反腐',\n",
       " '中国时速500公里动车组明日试验,将超越目前国内所有动车组速度;西南交大停电通知披露消息',\n",
       " '今日上午9时许,安顺一水泥厂在清理磨水渣仓时发生一起事故,造成2人死亡。',\n",
       " '曝托比亚斯-哈里斯与魔术队达成了4年6400万美元的续约协议,此前尼克斯和湖人等都对他感兴趣',\n",
       " '检方回应“秦皇岛亿元贪官母亲否认儿子贪污”:不可能,已掌握相关证据,待案件侦办完会向社会公布',\n",
       " '加勒比小国靠卖国籍吸金:中国、俄罗斯及中东富豪成推销对象,只要付出25万美元即可成为公民,凭此该国获得庞大收入。 ',\n",
       " '吉林市现“僵尸车”被困“牢笼”全身没漆',\n",
       " '广州14岁女初中生在校中毒身亡,警方调查怀疑系自杀;目击者称女生死前曾搅拌水杯。',\n",
       " '宝山金盛公寓居民逢下雨都犯愁屋顶渗水,天花板被腐蚀开裂,墙面霉迹斑斑;小区物业消失开发商失联。',\n",
       " '陌陌回应网易炮轰:唐岩称指控是恶意行为,将积极展开自我辩护;陌陌表示任何法律行动都会造成负面影响。',\n",
       " '组图:9-11恐怖袭击后,布什政府主要成员反应的照片公布,照片真实反映了美国高层官员的震惊和恐惧。',\n",
       " '哆啦A梦中最受关注的配角竟然是大雄的老师?!头条指数为你揭秘!',\n",
       " '结婚十周年妻子车祸丧生,丈夫空买钻戒无人可送;10岁儿子8岁女儿不知母亲已逝,仍画画作母亲节礼物',\n",
       " '人社部称要建立公务员基本工资标准正常调整机制;6月底前近1400万名公务员工资将落实调整。',\n",
       " '英超:曼联主场0-1南安普敦,塔迪奇打入制胜球;曼联10轮不败终结,南安普敦积分反超曼联重回前3',\n",
       " '苏州一男子因嫌发小太过粘人,冲动之下将其砸死抛尸树林;发小极度依赖超出“哥们”承受范围令其精神崩溃。',\n",
       " '组图:江苏无锡一女司机将奔驰车开入路边河道里,所幸及时爬出天窗逃生,被周围群众和消防官兵救出。',\n",
       " '浙江余姚外地居民与志愿者起冲突 救援物资遭哄抢',\n",
       " '据报道,郑州新郑机场一架飞机降落时坠地,机头下陷与地面摩擦出很大火花,滑行数几公里,120、119人员到达现场。',\n",
       " '岳阳:洞庭湖万亩湿地被高价拍卖,“候鸟天堂”被改造成养蟹池塘;拍卖时有人出540万,成交价却只有370万。',\n",
       " '中国证券业协会官网被黑,跳转到赌场,回应称已注意到该现象,正在处理解决',\n",
       " '解放军炮兵、步战车赴中缅边境集结现场曝光,空军等将实弹演习;专家称演习不是秀肌肉,并非针对缅政府',\n",
       " '河池市发布暴雨橙色预警:金城江区3小时降雨量已达50毫米且降雨持续。...',\n",
       " '扬州一家三口饭后中毒,10岁女儿身亡,警方不排除人为投毒;3人曾喝两亲戚买来饮料,两亲戚没喝无异常。',\n",
       " '琼海一孕妇腹痛去医院就诊,打完点滴后出现抽筋呕吐症状,两小时后胎死腹中;医生称死因需要由专家定论。',\n",
       " '保定蠡县“爆粗口”村支书被免,此前在受访时扬言扇死记者,称自己带的村委会班子“一个人好人没有”。',\n",
       " '四川一厅官落马前多次斥责举报者:你们是在反对共产党;曾多次晒与郭永祥、李崇禧等“大老虎”合照',\n",
       " '烟台:男子驾车撞六旬环卫工后弃车逃逸,环卫工当场身亡;警方3小时破案,抓获肇事司机,事故仍在调查中。',\n",
       " '四川古蔺现密集恐龙脚印化石,疑还原亿万年前猎杀场景;专家称脚印或是翼龙所留,有待进一步考证(组图)',\n",
       " '普京称,今年俄罗斯核力量将获得40多枚新型洲际弹道导弹,这些导弹能突破技术最完善的导弹防御系统。 ',\n",
       " '安庆:宿松县五里乡废弃屋内现六具尸骸追踪 诸多疑问待解',\n",
       " '501吨“神龟”落户曹妃甸,将成环渤海区域旅游文化新地标,本月11日前免费开放',\n",
       " '袁隆平回应安徽万亩“超级稻”减产绝收:推广多年品种有可能退化,出点小问题,不能说都有问题',\n",
       " '快讯:日本著名演员高仓健10日因病去世,享年83岁。',\n",
       " '“表妹”实为景春华情妇,开公司倒卖用地指标,后被带走;景任衡水书记后出门常警车开道。',\n",
       " '培生公司同意以7.3亿美元出售经济学人集团50%股份,买家为意大利阿涅利家族控股公司Exor。',\n",
       " '亚足联裁判委员会主席将由张吉龙担任,中国足协的多位候选人确定出任亚足联核心部门领导',\n",
       " '万达回应总部迁往上海:内部还未正式公开文件;王健林曾提出计划将上海万达广场从7个拓展到20个',\n",
       " '中央巡视组:文化部直属企业经营混乱,评奖过滥且存在暗箱操作;环保部领导开公司揽项目牟利;工商联专项资金使用存在问题。',\n",
       " '新西兰恒天然乳粉检出肉毒杆菌 部分流入中国',\n",
       " '松原直属库主任邓安福、开原直属库原主任尤连春等被行政记过、撤职;涉嫌犯罪移交司法部门',\n",
       " '临沂:女子集市遇抢劫,劫匪冒充其老公喊“还不回家”,围观人群以为夫妻打架未阻止。',\n",
       " '曝工行快捷支付有重大漏洞:犯罪分子强行给储户开通快捷支付,通过截获短信验证码盗取存款;6月以来多位北京储户存款被盗。',\n",
       " '达州:乡村“盗墓笔记”十年频现盗尸 24小时守墓仍被盗',\n",
       " '河北民警牺牲后妻子跳楼,两人系同学;儿子探望遗体哭晕,曾目睹母亲跳楼;媒体追问打死警察的枪哪里来。',\n",
       " '成都机场遇大雾,约9000名旅客行程受阻;预计延误航班将在16时恢复正常',\n",
       " '重庆年轻母亲为救4岁白血病女儿卖“拥抱”:10元抱一次,首日收600多元;回应市民质疑称实属无奈',\n",
       " '乐山:女子在大巴车被邻座男子多次抚摸,辩解不小心碰到;一怒之下扇其耳光:老娘30多岁,是摸还是碰能不知道?',\n",
       " '黔南布依族苗族自治州发布暴雨黄色预警: 根据最新监测资料分析,目前我州惠水、贵定、龙里个别乡镇降水量已达到50m...',\n",
       " '厦门:男子输光积蓄怪前女友破坏运气,持恋爱期间偷拍性爱视频敲诈前女友2000元;其已被判拘役3个月并处罚金2000元。',\n",
       " '海口:数百名村民“围攻”城管,致20余人受伤,疑村民有组织有预谋阻碍执法。',\n",
       " '南安蓉溪村出现兽爪印,目击者称野兽身上有花斑,应该是豹子;当地几十年前豹满为患,曾出动部队清理',\n",
       " '哈尔滨小伙远赴温州见女网友,发现对方身材相貌与照片判若两人,遂对其当街暴打,并于当日乘飞机离去',\n",
       " '9名遭菲扣押中国渔民昨日获释,被控非法捕捞海龟,因无力付罚金,在菲坐牢整1年。渔民牢中照片',\n",
       " '奥巴马昨日宣誓就职美国总统并发表演讲,重申“人人平等而自由”的美国精神,继续支持世界民主',\n",
       " '“打工皇帝”杨元庆年薪达1.33亿元,较上一年大幅上涨46%;联想回应称很合理。',\n",
       " '第一季度福特降低在华产量,以应对对手价格战和汽车市场的趋缓;4月长安福特大部分车型销量都同比下降。']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw_datasets[\"test\"][\"summary\"][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0c3c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "gold_list = []\n",
    "for text in raw_datasets[\"test\"][\"document\"][0:100]:\n",
    "    pred_list.append(generate_summary(text, model)[1][0])\n",
    "gold_list = list(raw_datasets[\"test\"][\"summary\"][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "592fc714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting lawrouge\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/cc/9c/cc411fd95b5fdf1924b2336f33d1eb304b65c029d77666315d4d8ff8ba0b/lawrouge-2.0.0.tar.gz (8.5 kB)\n",
      "Building wheels for collected packages: lawrouge\n",
      "  Building wheel for lawrouge (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lawrouge: filename=lawrouge-2.0.0-py3-none-any.whl size=9289 sha256=de9df0aaaabcba163b251af9c1ecfb236f58debf9d1b470430f74b09822f5daf\n",
      "  Stored in directory: /home/ma-user/.cache/pip/wheels/89/d6/bd/fafe523360e3233e6a58600caf802538dc04171fb32423f665\n",
      "Successfully built lawrouge\n",
      "Installing collected packages: lawrouge\n",
      "Successfully installed lawrouge-2.0.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-1.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install lawrouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2577f925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lawrouge\n",
    "rouge = lawrouge.Rouge()\n",
    "score = rouge.get_scores([\"他是清华大学计算机科学与技术系。计算机科学与技术专业。\"], [\"他是清华大学计算机科学与技术系。\"], avg=0)\n",
    "score[0][\"rouge-2\"][\"r\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d157824",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90819ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted score:  0.45293061867921885\n"
     ]
    }
   ],
   "source": [
    "import lawrouge\n",
    "rouge = lawrouge.Rouge()\n",
    "score_s = 0\n",
    "for i in range(len(gold_list)):\n",
    "    score = rouge.get_scores([pred_list[i].replace(\" \", \"\")], [gold_list[i].replace(\" \", \"\")], avg=0)\n",
    "    score_s += score[0][\"rouge-l\"][\"r\"]\n",
    "score_ave = score_s/len(gold_list)\n",
    "print('weighted score: ', score_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8c0ab56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused2][夜宵]章子怡示爱汪峰被恶搞爆粗口回击:扯nmb;谢娜当众掐黄磊女儿脖子被批:没素质没爱心;周杰伦女友昆凌:看新闻才知婚讯。女友:我也是看了新闻...(组图)。))知情情案。详细也是新闻:看了看了)。看了。看。组图合同[unused2]'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list[1].replace(\" \", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d612fec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f': 0.6274782560840833, 'p': 0.52, 'r': 0.7911111111111112}\n"
     ]
    }
   ],
   "source": [
    "rouge = lawrouge.Rouge()\n",
    "scores = rouge.get_scores([\"他是清华大学计算机科学与技术系\"], [\"计算机科学与技术专业\"], avg=2)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3625a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"test\"][\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adf101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moxing as mox\n",
    "mox.file.copy('/home/ma-user/work/obs_file.txt', 'obs://bart-summ/obs_file.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
